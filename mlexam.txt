ml1
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

data=pd.read_csv("uber.csv")

data.head(5)

data.isnull().sum()

data.dropna(inplace=True)

data['pickup_datetime']=pd.to_datetime(data['pickup_datetime'], errors='coerce')
data['pickup_hour']=data['pickup_datetime'].dt.hour
data['pickup_day']=data['pickup_datetime'].dt.dayofweek

data.drop(['pickup_datetime'], axis=1, inplace=True, errors='ignore') 

data['fare_amount']=pd.to_numeric(data['fare_amount'],errors='coerce')

data.dropna(subset=['fare_amount'],inplace=True)

plt.figure(figsize=(10,6))
sns.boxplot(x=data['fare_amount'])
plt.title("outlier detection for fare amount")
plt.show()

Q1 = data['fare_amount'].quantile(0.25)
Q3 = data['fare_amount'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

data = data[(data['fare_amount'] >= lower_bound) & (data['fare_amount'] <= upper_bound)]

numeric_data=data.select_dtypes(include=[np.number])

correlation_matrix=numeric_data.corr()

plt.figure(figsize=(12,8))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title("Correlation matrix")
plt.show()

x=data.select_dtypes(include=[np.number])
y=data['fare_amount']

x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=42)
linear_model=LinearRegression()
linear_model.fit(x_train, y_train)
y_pred_linear=linear_model.predict(x_test)
y_pred_linear
rf_model=RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(x_train, y_train)
y_pred_rf=rf_model.predict(x_test)

rmse_linear=np.sqrt(mean_squared_error(y_test, y_pred_linear))
rmse_linear
r2_linear=r2_score(y_test,y_pred_linear)
r2_linear

rmse_rf=np.sqrt(mean_squared_error(y_test, y_pred_rf))
rmse_rf
r2_rf=r2_score(y_test,y_pred_rf)
r2_rf


ml2
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler

df=pd.read_csv("emails.csv")

df.isnull().sum()

df.head(5)

x=df.drop(columns=['Email No.', 'Prediction'])
y=df['Prediction']

x=x.astype(float)

scaler=StandardScaler()
x_scaled=scaler.fit_transform(x)

x_train,x_test, y_train, y_test=train_test_split(x_scaled, y, test_size=0.2, random_state=42)

knn=KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train,y_train)
y_pred_knn=knn.predict(x_test)

print(classification_report(y_test, y_pred_knn))
print(accuracy_score(y_test,y_pred_knn))
print(confusion_matrix(y_test,y_pred_knn))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_knn, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title("KNN Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

svm=SVC(kernel='linear')
svm.fit(x_train,y_train)
y_pred_svm=svm.predict(x_test)

print(classification_report(y_test,y_pred_svm))
print(accuracy_score(y_test,y_pred_svm))
print(confusion_matrix(y_test,y_pred_svm))

conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_svm, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.title("SVM Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()



ml4
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, auc, precision_recall_curve

df=pd.read_csv("diabetes.csv")

df.isnull().sum()

x=df.drop(columns=['Outcome'])
y=df['Outcome']

x_train, x_test, y_train, y_test=train_test_split(x,y, test_size=0.2, random_state=42)

scaler=StandardScaler()
x_train=scaler.fit_transform(x_train)
x_test=scaler.transform(x_test)

print(f"X_train shape: {x_train.shape}")
print(f"y_train shape: {y_train.shape}")

knn=KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)
y_pred=knn.predict(x_test)

cm=confusion_matrix(y_test,y_pred)
cm

plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, 
            xticklabels=['Not Diabetic', 'Diabetic'], 
            yticklabels=['Not Diabetic', 'Diabetic'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

accuracy= accuracy_score(y_test,y_pred)
error_rate=1-accuracy
error_rate
precision_score(y_test,y_pred)
recall_score(y_test,y_pred)

plt.figure(figsize=(10,6))
plt.hist(y_test,bins=30, alpha=0.7, label='actual', color='blue')
plt.hist(y_pred, bins=30, alpha=0.7, label='predicted', color='red')
plt.legend(loc="best")
plt.title('actual vs predicted')
plt.xlabel("outcome")
plt.ylabel('frequency')
plt.show

ml5
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv("Mall_Customers.csv")

# Preview the dataset
print(data.head())

# Select features for clustering (Annual Income and Spending Score)
X = data[['Annual Income (k$)', 'Spending Score (1-100)']]

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Calculate WCSS (Within-Cluster Sum of Squares) for different values of k
wcss = []
K = range(1, 11)  # Try k values from 1 to 10

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)  # Inertia is the WCSS

# Plot the Elbow Method using WCSS
plt.figure(figsize=(8, 6))
plt.plot(K, wcss, 'bo-', markersize=8)
plt.xlabel("Number of Clusters (k)")
plt.ylabel("WCSS (Within-Cluster Sum of Squares)")
plt.title("Elbow Method using WCSS")
plt.show()

# Define the optimal number of clusters from the elbow plot (e.g., k=5)
k_optimal = 5

# Apply K-Means with the optimal number of clusters
kmeans_optimal = KMeans(n_clusters=k_optimal, random_state=42)
data['Cluster'] = kmeans_optimal.fit_predict(X_scaled)

# Plot the clusters
plt.figure(figsize=(10, 8))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=data['Cluster'], cmap='viridis', s=50, alpha=0.7)
plt.xlabel("Annual Income (scaled)")
plt.ylabel("Spending Score (scaled)")
plt.title(f"Customer Segments (K-Means Clustering with k={k_optimal})")
plt.colorbar(label="Cluster")
plt.show()



ML3
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import accuracy_score, confusion_matrix

# Load the dataset
data = pd.read_csv('Churn_Modelling.csv')

# Drop irrelevant columns
data_cleaned = data.drop(columns=['RowNumber', 'CustomerId', 'Surname'])

# Encode categorical columns
label_encoder_gender = LabelEncoder()
data_cleaned['Gender'] = label_encoder_gender.fit_transform(data_cleaned['Gender'])

label_encoder_geo = LabelEncoder()
data_cleaned['Geography'] = label_encoder_geo.fit_transform(data_cleaned['Geography'])

# Separate features (X) and target (y)
X = data_cleaned.drop('Exited', axis=1)
y = data_cleaned['Exited']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the neural network
model = Sequential()

# Input layer and first hidden layer (using ReLU activation)
model.add(Dense(units=10, activation='relu', input_dim=X_train_scaled.shape[1]))
model.add(Dense(units=8, activation='relu'))

# Output layer (using sigmoid activation for binary classification)
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_split=0.2)  # Corrected validation_split

# Make predictions on the test set
y_pred_prob = model.predict(X_test_scaled)
y_pred = (y_pred_prob > 0.5).astype(int)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Print results
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)


import seaborn as sns
import matplotlib.pyplot as plt

conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
